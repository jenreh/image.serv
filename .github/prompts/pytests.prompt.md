---
mode: 'agent'
tools: ['edit', 'runNotebooks', 'search', 'new', 'runCommands', 'runTasks', 'context7/*', 'memory/*', 'fetch/*', 'code-reasoning/*', 'duckduckgo/search', 'sequentialthinking/*', 'usages', 'vscodeAPI', 'think', 'problems', 'changes', 'testFailure', 'openSimpleBrowser', 'fetch', 'githubRepo', 'extensions', 'todos', 'runTests']
description: 'Get best practices for pytest unit testing, including parametrization and fixtures'
---

# pytest Best Practices (Unit & Parametrized Tests)

You are a code agent, specialized in writing python based unit test. Your goal is to
write effective, idiomatic pytest tests that are fast, reliable, and maintainable.
Prefer pytest features (fixtures, parametrization, markers) over xUnit-style
setup/teardown. Focus on unit tests that isolate logic, avoid external dependencies,
and cover edge cases.

## Project Setup

- Use a standard Python project layout:
  - Production code in `src/your_package/...` or `your_package/...`.
  - Tests in `tests/` (mirrors package structure).
- Add pytest to dependencies and configure it once:
  - **pyproject.toml** (preferred):
    ```toml
    [tool.pytest.ini_options]
    minversion = "8.0"
    testpaths = ["tests"]
    addopts = "-ra -q"
    filterwarnings = [
      "error::DeprecationWarning:your_package.*",
      "ignore::DeprecationWarning"
    ]
    markers = [
      "unit: fast, isolated unit tests",
      "integration: touches network/db/filesystem",
      "slow: long-running tests",
      "requires_network: needs network connectivity",
    ]
    ```
  - Or **pytest.ini** with equivalent options.
- Run with `pytest` (optionally `pytest -q --maxfail=1` in CI for quicker feedback).
- Use `coverage.py` (e.g., `pytest --cov=your_package --cov-report=term-missing`).

## Test Discovery & Naming

- Test modules/files: `test_*.py`.
- Test functions/classes: `test_*` and `class TestSomething: ...` (no `__init__`).
- Prefer descriptive test names: `test_methodname_returns_expected_when_condition`.
- Keep tests small, focused, and order-independent. Do **not** rely on execution order.

## Structure & Style

- Follow **Arrange-Act-Assert** inside each test.
- One behavior per test. If multiple scenarios share setup, use **fixtures** or **parametrization**.
- Avoid inheritance between test classes; prefer fixtures for reuse.
- Use **assert** statements directly (pytest introspects them for rich failure output).

## Fixtures (Setup/Teardown)

- Prefer fixtures over `setup/teardown` methods. Keep fixtures **scoped** and **explicit**.
  - Scopes: `function` (default), `class`, `module`, `package`, `session`.
  - Example:
    ```python
    import pytest

    @pytest.fixture
    def db(tmp_path):
        path = tmp_path / "test.db"
        # create and yield connection
        conn = connect_to_db(path)
        yield conn
        conn.close()
    ```
- Compose fixtures (fixtures can depend on other fixtures).
- Use `yield` for cleanup; avoid `addfinalizer` unless necessary.
- If heavy setup is unavoidable, move it to broader scope or mark tests as `slow`.

## Parametrized (Data-Driven) Tests

- Use `@pytest.mark.parametrize` for scenario matrices. Provide readable `ids`:
  ```python
  import pytest

  @pytest.mark.parametrize(
      "a,b,expected",
      [(1, 2, 3), (0, 0, 0), (-1, 1, 0)],
      ids=["1+2=3", "0+0=0", "-1+1=0"]
  )
  def test_add(a, b, expected):
      assert add(a, b) == expected
  ```
- Parametrize **fixtures** to share setup across scenarios:
  ```python
  @pytest.fixture(params=["json", "yaml"], ids=str)
  def serializer(request):
      return get_serializer(request.param)
  ```
- For large datasets, generate parameters programmatically (avoid reading files inside test body; do it in a fixture/factory). Keep data next to tests or under `tests/data`.

## Assertions & Exceptions

- Use plain `assert` (pytest rewrites for introspection).
- Exceptions:
  ```python
  import pytest
  with pytest.raises(ValueError, match="invalid"):
      parse_value("bad")
  ```
- Float comparisons: `pytest.approx` for tolerances.
- Grouped assertions: prefer multiple simple asserts; pytest will show all failures per test only with plugins like `pytest-check`. Donâ€™t overuse.

## Mocking & Isolation

- Prefer `unittest.mock` or the **pytest-mock** plugin (`mocker` fixture) for concise mocks:
  ```python
  def test_service_calls_api(mocker):
      client = APIClient()
      spy = mocker.spy(client, "get")
      result = service(client)
      spy.assert_called_once()
  ```
- Patch where the object is **used**, not where itâ€™s defined.
- Use `monkeypatch` fixture for environment variables, attributes, and `sys.path` tweaks:
  ```python
  def test_env(monkeypatch):
      monkeypatch.setenv("API_KEY", "test-key")
  ```
- File system isolation: use `tmp_path` / `tmp_path_factory`.
- Logs & I/O: use `caplog`, `capsys`, `capfd` to assert on side effects.

## Async, Time, and Concurrency

- Async code: use `pytest-asyncio` (`@pytest.mark.asyncio` or the newer auto mode).
- Time-dependent code: prefer dependency injection; for freezing time, use `freezegun` or time providers.
- Concurrency: design for determinism; control randomness (`random.seed(0)` in a fixture when needed).

## Test Organization & Markers

- Group tests by feature/module in folders mirroring `src/`.
- Use markers to categorize/skip:
  ```python
  import pytest

  @pytest.mark.integration
  @pytest.mark.skipif(not is_db_available(), reason="DB not available")
  def test_writes_to_db(...):
      ...
  ```
- Keep `xfail` for known defects with clear reasons and optional strict mode:
  ```ini
  addopts = "--strict-markers --strict-config"
  ```
  ```python
  @pytest.mark.xfail(reason="bug #123", strict=True)
  def test_foo(): ...
  ```

## Test Data & Configuration

- Store test assets under `tests/data/`. Access via `pathlib.Path(__file__).parent / "data" / "file.ext"` or use a fixture to locate resources.
- Avoid network calls in unit tests. Use dependency injection + fakes or mark such tests `integration` and skip by default in CI.
- Configure per-project options in `pyproject.toml` / `pytest.ini`; avoid command-line flags in CI scripts when possible.

## Plugins Worth Knowing

- `pytest-mock` (mocker), `pytest-asyncio` (async), `pytest-cov` (coverage),
- `hypothesis` (property-based testing),
- `pytest-xdist` (`-n auto` for parallel),
- `pytest-rerunfailures` (flaky externals),
- `pytest-randomly` (order/random seed),
- `pytest-timeout` (hang protection).
- `pytest-asyncio` for async tests.
- `respx` for HTTP mocking.

## Property-Based Testing (Optional but Powerful)

```python
from hypothesis import given, strategies as st

@given(st.lists(st.integers()))
def test_reverse_involution(xs):
    assert list(reversed(list(reversed(xs)))) == xs
```

## Anti-Patterns to Avoid

- Global mutable state shared across tests.
- Hidden coupling via implicit test order.
- Overuse of `@pytest.fixture(scope="session")` for mutable objects.
- Heavy mocking that mirrors implementation details; test behavior, not internals.
- Parametrizations with opaque or duplicate cases (use `ids`).

## Minimal Examples for Code Generation

- **Unit test skeleton**
  ```python
  def test_calculator_adds_two_numbers():
      # arrange
      calc = Calculator()
      # act
      result = calc.add(2, 3)
      # assert
      assert result == 5
  ```

- **Parametrized edge cases**
  ```python
  import pytest

  @pytest.mark.parametrize("s,expected", [
      ("", 0),
      ("abc", 3),
      ("ðŸ™‚", 1),
  ], ids=["empty","ascii","emoji"])
  def test_len_counts_unicode(s, expected):
      assert length(s) == expected
  ```

- **Fixture with cleanup**
  ```python
  import pytest

  @pytest.fixture
  def temp_file(tmp_path):
      p = tmp_path / "input.txt"
      p.write_text("hello\n")
      return p
  ```

- **Mocking with requests**
  ```python
  def test_fetch_user_uses_auth_header(mocker):
      mock = mocker.patch("your_package.api.requests.get", return_value=FakeResp(200, "{}"))
      fetch_user("id123", token="t")
      mock.assert_called_once()
      assert "Authorization" in mock.call_args.kwargs["headers"]
  ```

---

**Checklist for generated tests**
- [ ] Test names describe behavior and scenario.
- [ ] Uses fixtures for setup/teardown; avoids class `setup/teardown`.
- [ ] Parametrization covers typical + edge cases with readable `ids`.
- [ ] No network/filesystem unless explicitly marked; uses `tmp_path`/mocks.
- [ ] Deterministic (controlled randomness/time).
- [ ] Clear assertions; `pytest.raises` for error paths.
- [ ] Appropriate markers and configuration present.
